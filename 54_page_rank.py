# SEARCH ENGINE ARCHITECTURE

# Web Crawler - program that hits a web page, parses the html, makes a queue of all the linked pages and repeats until the queue is empty.
# robots.txt is a file present in all website roots; it tells web crawlers which pages are allowed / disallowed.

# Index - data that is collected, parsed and stored so that searches can be fast and accurate. Without an index, the search engine would scan every document in the corpus, which would require vast amounts of time and computing power.
# Pagerank algorithm - determines the quality of a webpage by the number of quality links to it, repeat. The calculation has to run a number of times in order to reach a converging result.

# Search (not part of this lesson's scope) - finds the top pages related to the searched keywords